{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.0.5"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This R environment comes with many helpful analytics packages installed\n# It is defined by the kaggle/rstats Docker image: https://github.com/kaggle/docker-rstats\n# For example, here's a helpful package to load\n\nlibrary(tidyverse) # metapackage of all tidyverse packages\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nlist.files(path = \"../input\")\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-24T12:23:08.618999Z","iopub.execute_input":"2021-12-24T12:23:08.621304Z","iopub.status.idle":"2021-12-24T12:23:09.968500Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Importing the libraries","metadata":{}},{"cell_type":"code","source":"# Using advanced regression techniques to perform analysis on used car dataset\noptions(warn=-1)\n# Importing libraries\nlibrary(caret)\nlibrary(glmnet)\nlibrary(ggplot2)\nlibrary(ranger)\nlibrary(e1071)\nset.seed(1234)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-24T12:23:09.971021Z","iopub.execute_input":"2021-12-24T12:23:10.008872Z","iopub.status.idle":"2021-12-24T12:23:12.332223Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Importing data sets","metadata":{}},{"cell_type":"code","source":"X_train <- read.csv2('../input/used-car-price-dataset-competition-format/X_train.csv', sep = ',')\nX_test <- read.csv('../input/used-car-price-dataset-competition-format/X_test.csv', sep = ',')\ny_train <- read.csv('../input/used-car-price-dataset-competition-format/y_train.csv', sep = ',')\ny_test <- read.csv('../input/used-car-price-dataset-competition-format/test_label/y_test.csv', sep = ',')","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:12.334969Z","iopub.execute_input":"2021-12-24T12:23:12.336644Z","iopub.status.idle":"2021-12-24T12:23:12.447373Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis ","metadata":{}},{"cell_type":"code","source":"# We bind all the data together and then reshuffle it to remove any splitting bias beforehand.\n# Merging with carID as it is unique\ntrain_full <- merge(X_train, y_train, by = 'carID')\ntest_full <- merge(X_test, y_test, by = 'carID')\n\nfull_data <- rbind(train_full, test_full)\n\n# Removing carID as it doesn't serve any useful purpose in our analysis\nfull_data <- select(full_data, -carID)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:12.449842Z","iopub.execute_input":"2021-12-24T12:23:12.451344Z","iopub.status.idle":"2021-12-24T12:23:12.508995Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"head(full_data)  # We see that some columns do not have correct data types","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:12.511513Z","iopub.execute_input":"2021-12-24T12:23:12.513044Z","iopub.status.idle":"2021-12-24T12:23:12.544638Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"anyNA(full_data)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:12.547103Z","iopub.execute_input":"2021-12-24T12:23:12.548592Z","iopub.status.idle":"2021-12-24T12:23:12.565814Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# We change the data types according to the data \nfull_data <- mutate_at(full_data, c(7,8,9), as.double)  # Changing character to double\nfull_data <- full_data %>% mutate_if(is.character, ~as.factor(.))   # Changing character to factors as they are categories\nfull_data <- mutate_at(full_data, c(3), as.factor) # Changing year as a factor","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:12.568999Z","iopub.execute_input":"2021-12-24T12:23:12.570699Z","iopub.status.idle":"2021-12-24T12:23:12.631699Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"head(full_data)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:12.635796Z","iopub.execute_input":"2021-12-24T12:23:12.637679Z","iopub.status.idle":"2021-12-24T12:23:12.670587Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"dim(full_data)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:12.674504Z","iopub.execute_input":"2021-12-24T12:23:12.676294Z","iopub.status.idle":"2021-12-24T12:23:12.695459Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"library(repr)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:12.699230Z","iopub.execute_input":"2021-12-24T12:23:12.700865Z","iopub.status.idle":"2021-12-24T12:23:12.724716Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"fig_size <- function(width, height){\n    options(repr.plot.width=width, repr.plot.height = height)\n}","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:12.743042Z","iopub.execute_input":"2021-12-24T12:23:12.744729Z","iopub.status.idle":"2021-12-24T12:23:12.758358Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"brands <- full_data %>% group_by(brand) %>% summarise(length(brand))","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:12.762630Z","iopub.execute_input":"2021-12-24T12:23:12.764556Z","iopub.status.idle":"2021-12-24T12:23:12.794548Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"fig_size(16,9)\n\nggplot(data = brands, aes(x = brand, y = `length(brand)`)) + \n  geom_bar( stat= 'identity', width=0.6, fill = '#9999FF', color='black') + \n  geom_text(aes(label = `length(brand)` ), vjust=2,size = 5) +\n  xlab('Brand') + \n  ylab('Frequency') + \n  ggtitle('Number of cars of each brand') +\n  theme(panel.grid.major = element_blank(), \n        panel.background = element_rect('white'),\n        text = element_text(size = 20),\n        plot.title = element_text(hjust = 0.5))\n\n# Number of cars from each brand in the dataset\n","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:12.798589Z","iopub.execute_input":"2021-12-24T12:23:12.800236Z","iopub.status.idle":"2021-12-24T12:23:13.390799Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"bybrand <- full_data %>% group_by(brand, transmission) %>% summarise(price = mean(price))","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:13.393317Z","iopub.execute_input":"2021-12-24T12:23:13.394850Z","iopub.status.idle":"2021-12-24T12:23:13.422432Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"fig_size(20,8)\nggplot(data = bybrand, aes(x = brand, y = price, fill= transmission))+\n  geom_bar(stat='identity', position = position_dodge2(),width=0.7, color='black') +\n  xlab('Brand')+\n  ylab('Average price of cars') +\n  scale_fill_brewer()+\n  ggtitle('Average price of cars for different transmissions and brands')+\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.background = element_rect('white'),\n        text = element_text(size = 20),\n        plot.title = element_text(hjust = 0.5))\n\n# On an average, semi-auto version in Audi is the costliest\n","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:13.425968Z","iopub.execute_input":"2021-12-24T12:23:13.427573Z","iopub.status.idle":"2021-12-24T12:23:13.962630Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"taxincur <- full_data %>% group_by(fuelType) %>% summarise(tax = tax)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:13.966911Z","iopub.execute_input":"2021-12-24T12:23:13.969183Z","iopub.status.idle":"2021-12-24T12:23:13.995916Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"ggplot(data = taxincur, aes(x = fuelType, y = tax))+\n  geom_violin(fill = '#FFCC00', color='black') +\n  xlab('Fuel Type')+\n  ylab('Tax') +\n  ggtitle('Tax distribution for different Fuel Type') +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.background = element_rect('white'),\n        text = element_text(size = 20),\n        plot.title = element_text(hjust = 0.5))\n\n# As we see, taxes are very strict for petrol and diesel cars and almost zero for electric cars","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:13.999870Z","iopub.execute_input":"2021-12-24T12:23:14.001538Z","iopub.status.idle":"2021-12-24T12:23:14.457937Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"byfuel <- full_data %>% group_by(brand, fuelType) %>% summarise(price = mean(price))\n","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:14.460487Z","iopub.execute_input":"2021-12-24T12:23:14.461983Z","iopub.status.idle":"2021-12-24T12:23:14.486966Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"fig_size(20,9)\nggplot(data = byfuel, aes(x = brand, y = price, fill=fuelType)) +\ngeom_bar(stat = 'identity', position = position_dodge2(), width=0.7, color = 'black')+\nxlab('Brand')+\nylab('Average price of cars') +\nscale_fill_brewer(palette = 'Pastel2') +\nggtitle('Average price for different Fuel Types')+\ntheme(panel.grid.major = element_blank(), \n        panel.background = element_rect('white'),\n        text = element_text(size = 20),\n        plot.title = element_text(hjust = 0.5))\n\n# Prices for Petrol cars is highest on an average, BMW is the only brand with electric cars","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:14.489520Z","iopub.execute_input":"2021-12-24T12:23:14.491091Z","iopub.status.idle":"2021-12-24T12:23:14.971222Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"years <- full_data %>% group_by(year) %>% summarise(length(year))","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:14.973852Z","iopub.execute_input":"2021-12-24T12:23:14.975547Z","iopub.status.idle":"2021-12-24T12:23:14.994368Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"fig_size(18,10)\nggplot(data = years, aes(x = year, y = `length(year)`)) + \n  geom_bar(stat = 'identity', width=0.6, fill = '#FFCCCC', color='black') + \n  geom_text(aes(label = `length(year)`), vjust = -1, size = 5 ) +\n  xlab('Year') + \n  ylab('Frequency') +\n  ggtitle('Car sales in each year') + \n  theme(panel.grid.major = element_blank(), \n        panel.background = element_rect('white'),\n        text = element_text(size = 20),\n        plot.title = element_text(hjust = 0.5))\n\n# Most of the cars were bought in 2019 ","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:14.998502Z","iopub.execute_input":"2021-12-24T12:23:15.000166Z","iopub.status.idle":"2021-12-24T12:23:15.378401Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Let's see which fuelType provides the most miles per gallon and how are they priced\nbyfuelmpg <- full_data %>% group_by(fuelType) %>% summarize(mpg = mean(mpg), price = mean(price))\nbyfuelmpg","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:15.383006Z","iopub.execute_input":"2021-12-24T12:23:15.385289Z","iopub.status.idle":"2021-12-24T12:23:15.432353Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"fig_size(17,9)\nggplot(data = byfuelmpg, aes(x= mpg, y = price, fill=fuelType)) +\ngeom_point(size = 12, alpha = 0.5, shape=21, color='black') +\nxlab('Price') +\nylab('Miles per gallon') +\nscale_fill_brewer(palette = 'Spectral') +\nggtitle('Average price for different Fuel Types and their mpg')+\ntheme(panel.grid.major = element_blank(), \n        panel.background = element_rect('white'),\n        text = element_text(size = 20),\n        plot.title = element_text(hjust = 0.5))\n\n# We see that elecrtic cars have the lowest average price but highest efficiency, maybe we have less data points to support more information.\n# Also, Diesel cars most costly with the lowest mpg\n","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:15.436345Z","iopub.execute_input":"2021-12-24T12:23:15.438096Z","iopub.status.idle":"2021-12-24T12:23:15.878207Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"head(full_data)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:15.882417Z","iopub.execute_input":"2021-12-24T12:23:15.884829Z","iopub.status.idle":"2021-12-24T12:23:15.914120Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Let's check out some continuos variables and their distributions\nfig_size(16,8)\nggplot(data = full_data, aes(x=engineSize, y=price)) +\ngeom_point(aes(size=price, color=mpg), alpha = 0.7)+\ngeom_smooth(method='lm', se=T, color ='red') +\nxlab('Engine Size')+ \nscale_fill_brewer(palette = 'Set2')+\nggtitle('Price according to Engine Size scaled by mpg')+\nylab('Price') +\ntheme(panel.grid.major = element_blank(), \n        panel.background = element_rect('white'),\n        text = element_text(size = 20),\n        plot.title = element_text(hjust = 0.5))\n\n# It looks like engines with really less size have relatively more average than heavy engines and are less costly. Heavier the engine, more the price","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:15.916716Z","iopub.execute_input":"2021-12-24T12:23:15.918343Z","iopub.status.idle":"2021-12-24T12:23:17.099417Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Let's check out some continuos variables and their distributions\nfig_size(16,8)\nggplot(data = full_data, aes(x=mileage, y=price)) +\ngeom_point(color='#336600', shape=21, alpha = 0.5, size=1)+\nxlab('Mileage')+ \nscale_fill_distiller()+\nylab('Price') +\nggtitle('Price vs Mileage')+\ntheme(panel.grid.major = element_blank(), \n        panel.background = element_rect('white'),\n        text = element_text(size = 20),\n        plot.title = element_text(hjust = 0.5))\n\n# The graph doesn't look good, let's check the mileage and price columns seperately","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:17.101963Z","iopub.execute_input":"2021-12-24T12:23:17.103560Z","iopub.status.idle":"2021-12-24T12:23:17.676898Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"ggplot(data=full_data,aes(x=(mpg)))+\ngeom_histogram(aes(y=..density..), fill='#FFFFCC', color='black')+\ngeom_density(alpha=0.2,fill=\"#FFCCCC\")+\nxlab('mpg')+\nggtitle('Distribution of mpg')+\ntheme(panel.grid.major = element_blank(), \n        panel.background = element_rect('white'),\n        text = element_text(size = 20),\n        plot.title = element_text(hjust = 0.5))","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:17.679532Z","iopub.execute_input":"2021-12-24T12:23:17.681135Z","iopub.status.idle":"2021-12-24T12:23:18.076755Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"### Log Transformation needed, But why? \n\nYou can see that the center case (y) has been transformed to symmetry, while the more mildly right skew case (x) is now somewhat left skew. One the other hand, the most skew variable (z) is still (slightly) right skew, even after taking logs.\n\nIf we wanted our distributions to look more normal, the transformation definitely improved the second and third case. We can see that this might help.\n\nTaking logs \"pulls in\" more extreme values on the right (high values) relative to the median, while values at the far left (low values) tend to get stretched back, further away from the median. ","metadata":{}},{"cell_type":"markdown","source":"![log transformations](https://i.stack.imgur.com/7iSYs.png)","metadata":{}},{"cell_type":"code","source":"ggplot(data=full_data,aes(x=log(mpg)))+\ngeom_histogram(aes(y=..density..), fill='#FF9966', color='black')+\ngeom_density(alpha=0.2,fill=\"#FFFF66\")+\nxlab('mpg')+\nggtitle('Distribution of Log mpg')+\ntheme(panel.grid.major = element_blank(), \n        panel.background = element_rect('white'),\n        text = element_text(size = 20),\n        plot.title = element_text(hjust = 0.5))","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:18.080526Z","iopub.execute_input":"2021-12-24T12:23:18.082212Z","iopub.status.idle":"2021-12-24T12:23:18.763345Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"ggplot(data=full_data,aes(x=mileage))+\ngeom_histogram(aes(y=..density..), fill='#FFFFCC', color='black')+\ngeom_density(alpha=0.2,fill=\"#FFCCCC\")+\nxlab('Mileage')+\nggtitle('Distribution  of mileage')+\ntheme(panel.grid.major = element_blank(), \n        panel.background = element_rect('white'),\n        text = element_text(size = 20),\n        plot.title = element_text(hjust = 0.5))\n\n# It looks positively skewed and that is a problem as it will mess with our prediction error but we can solve it with log transformation","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:18.765851Z","iopub.execute_input":"2021-12-24T12:23:18.767493Z","iopub.status.idle":"2021-12-24T12:23:19.093470Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{"execution":{"iopub.status.busy":"2021-12-23T21:43:46.382873Z","iopub.execute_input":"2021-12-23T21:43:46.38427Z","iopub.status.idle":"2021-12-23T21:43:46.75028Z"}}},{"cell_type":"code","source":"ggplot(data=full_data,aes(x=log(mileage)))+\ngeom_histogram(aes(y=..density..), fill='#FF9966', color='black')+\ngeom_density(alpha=0.3,fill=\"#FFFF66\")+\nxlab('Mileage')+\nxlim(c(0,15))+\nggtitle('Distribution of Log mileage')+\ntheme(panel.grid.major = element_blank(), \n        panel.background = element_rect('white'),\n        text = element_text(size = 20),\n        plot.title = element_text(hjust = 0.5))","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:19.095837Z","iopub.execute_input":"2021-12-24T12:23:19.097270Z","iopub.status.idle":"2021-12-24T12:23:19.469991Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"###  It doesn't make our data perfectly normal but that will mostly not happen in practice. I used other transformations also like sqrt, cubic root but this gives the maximum R square value and since model inference is not our objective here, we can use transformations without the primary aim of interpreting our models and focus more on predictions.","metadata":{}},{"cell_type":"code","source":"ggplot(data=full_data,aes(x=price))+\ngeom_histogram(aes(y=..density..), fill='#FFFFCC', color='black')+\ngeom_density(alpha=0.2,fill=\"#FFCCCC\")+\nxlab('Price')+\nggtitle('Distribution of Price')+\ntheme(panel.grid.major = element_blank(), \n        panel.background = element_rect('white'),\n        text = element_text(size = 20),\n        plot.title = element_text(hjust = 0.5))","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:19.472584Z","iopub.execute_input":"2021-12-24T12:23:19.474052Z","iopub.status.idle":"2021-12-24T12:23:19.806136Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"ggplot(data=full_data,aes(x=log(price)))+\ngeom_histogram(aes(y=..density..), fill='#FF9966', color='black')+\ngeom_density(alpha=0.2,fill=\"#FFFF66\")+\nxlab('Price')+\nggtitle('Distribution of Log Price')+\ntheme(panel.grid.major = element_blank(), \n        panel.background = element_rect('white'),\n        text = element_text(size = 20),\n        plot.title = element_text(hjust = 0.5))","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:19.808596Z","iopub.execute_input":"2021-12-24T12:23:19.810065Z","iopub.status.idle":"2021-12-24T12:23:20.157618Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"# Time to build our models","metadata":{}},{"cell_type":"markdown","source":"### Data Transformations","metadata":{}},{"cell_type":"code","source":"# Price of the used cars seems to be positively skewed and I think a simple log transformation should balance the distribution.\nfull_data$price <- log(full_data$price)\n\n# Also some of the predictors in our training data are skewed, also it's in different scale, so we can apply the same technique there\n\nfull_data$mileage <- log(full_data$mileage)\n\n# We also have skewed mpg\nfull_data$mpg <- log(full_data$mpg)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:20.160046Z","iopub.execute_input":"2021-12-24T12:23:20.161516Z","iopub.status.idle":"2021-12-24T12:23:20.180740Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"### Seperate into training and test sets","metadata":{}},{"cell_type":"code","source":"id <- sample(nrow(full_data), floor(nrow(full_data)*0.75))\n\ntrain <- full_data[id,]\ntest <- full_data[-id,]\n\ntest_response <- as.numeric(test[, 'price'])\n\ntest <- select(test, -price)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:20.183143Z","iopub.execute_input":"2021-12-24T12:23:20.184615Z","iopub.status.idle":"2021-12-24T12:23:20.212253Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Defining R-Squared function for measuring performance of our predictive models.\nr_sqaured <- function(pred){\n  rss <- sum((pred - test_response) ^ 2)\n  tss <- sum((test_response - mean(test_response)) ^ 2)\n  return(round(1-rss/tss, 3))\n}","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:20.214737Z","iopub.execute_input":"2021-12-24T12:23:20.216223Z","iopub.status.idle":"2021-12-24T12:23:20.229561Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"### Using caret library for cross validation and model selection","metadata":{}},{"cell_type":"code","source":"# Setting the resampling techniques using caret's trainControl function\nfit_control <- trainControl(method = 'repeatedcv',\n                            number = 10,\n                            repeats = 4)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:20.231981Z","iopub.execute_input":"2021-12-24T12:23:20.233408Z","iopub.status.idle":"2021-12-24T12:23:20.247106Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"### Simple Linear Regression","metadata":{}},{"cell_type":"code","source":"# Building simple linear regression model using caret's train function\nsimplelrfit <- train(price~.,\n                     data = train,\n                     method = 'lm',\n                     trControl = fit_control)\npredsimplelrfit <- predict.train(simplelrfit, \n                                 newdata = test)\n\nrsq_simplelrfit <- r_sqaured(pred = predsimplelrfit)\n\n# After looking at the summary of the linear regression, we see that tax column in our datset \n#is not at all significant and so we drop it and actually it does not change any acuuracy value for any models which confirms this hypothesis.","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:20.249640Z","iopub.execute_input":"2021-12-24T12:23:20.251139Z","iopub.status.idle":"2021-12-24T12:23:34.485509Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"cat('Simple Linear Regression R-Square - ', rsq_simplelrfit)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:34.490306Z","iopub.execute_input":"2021-12-24T12:23:34.493263Z","iopub.status.idle":"2021-12-24T12:23:34.521699Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"pred_plot <- plot(x = test_response, \n                  y = predsimplelrfit, \n                  xlab = 'Actual Values',\n                  ylab = 'Predicted Values',\n                  main = 'Simple Linear Regression',\n                  ylim = c(6,12),\n                  abline(a=0,b=1, col = 'red'))\n\nresid_simplelr <- plot(x = predsimplelrfit,\n                       y = predsimplelrfit - test_response,\n                       abline(0,0,col = 'red'),\n                       xlab = 'Predicted Values',\n                       ylab = 'Residuals',\n                       main = 'Simple Linear Regression Residuals')","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:34.526762Z","iopub.execute_input":"2021-12-24T12:23:34.529785Z","iopub.status.idle":"2021-12-24T12:23:35.019603Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"### Ridge regression","metadata":{}},{"cell_type":"code","source":"# Now we try ridge regression to train our model and see how it works\n\nridgefit <- train(price~.,\n                     data = train,\n                     method = 'glmnet',\n                     trControl = fit_control,\n                     tuneGrid = expand.grid(data.frame(alpha = 0,\n                                                       lambda = seq(0.0001, 0.1,0.001))))\n\n# Optimal lambda chosen by the model is 0.0381, let's train using optimal lambda and measure our metric\n\noptridgefit <- train(price~.,\n                  data = train,\n                  method = 'glmnet',\n                  trControl = fit_control,\n                  tuneGrid = expand.grid(data.frame(alpha = 0,\n                                                    lambda = 0.0381)))\n\npredridgefit <- predict.train(ridgefit, newdata = test)\n\nrsq_ridgefit <- r_sqaured(predridgefit)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:35.022156Z","iopub.execute_input":"2021-12-24T12:23:35.023738Z","iopub.status.idle":"2021-12-24T12:23:49.601553Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"cat('Ridge Regression R-Square - ', rsq_ridgefit)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:49.604254Z","iopub.execute_input":"2021-12-24T12:23:49.606094Z","iopub.status.idle":"2021-12-24T12:23:49.621645Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"pred_plot <- plot(x = test_response, \n                  y = predridgefit, \n                  xlab = 'Actual Values',\n                  ylab = 'Predicted Values',\n                  main = 'Ridge Regression',\n                  ylim = c(6,12),\n                  abline(a=0,b=1, col = 'red'))\n\nresid_simplelr <- plot(x = predridgefit,\n                       y = predridgefit - test_response,\n                       abline(0,0,col = 'red'),\n                       xlab = 'Predicted Values',\n                       ylab = 'Residuals',\n                       main = 'Ridge Regression Residuals')","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:49.624163Z","iopub.execute_input":"2021-12-24T12:23:49.625657Z","iopub.status.idle":"2021-12-24T12:23:50.067532Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"### Lasso Regression","metadata":{}},{"cell_type":"code","source":"# Now we try Lasso regression to train our model and see how it works \n\nlassofit <- train(price~.,\n                  data = train,\n                  method = 'glmnet',\n                  trControl = fit_control,\n                  tuneGrid = expand.grid(data.frame(alpha = 1,\n                                                    lambda = seq(0.00001, 0.01,0.0001))))\n\n# Optimal lambda was given by the model is 0.00031, let's train our model using optimal lambda and measure our model\n\noptlassofit <- train(price~.,\n                  data = train,\n                  method = 'glmnet',\n                  tuneGrid = expand.grid(data.frame(alpha = 1,\n                                                    lambda = 0.00021)))\n\npredlassofit <- predict.train(optlassofit, newdata = test)\n\nrsq_lassofit <- r_sqaured(predlassofit)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:23:50.070272Z","iopub.execute_input":"2021-12-24T12:23:50.072089Z","iopub.status.idle":"2021-12-24T12:24:02.675147Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"cat('Ridge Regression R-Square - ', rsq_lassofit)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:24:02.677597Z","iopub.execute_input":"2021-12-24T12:24:02.679082Z","iopub.status.idle":"2021-12-24T12:24:02.693414Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"pred_plot <- plot(x = test_response, \n                  y = predlassofit, \n                  xlab = 'Actual Values',\n                  ylab = 'Predicted Values',\n                  main = 'Lasso Regression',\n                  ylim = c(6,12),\n                  abline(a=0,b=1, col = 'red'))\n\nresid_simplelr <- plot(x = predlassofit,\n                       y = predlassofit - test_response,\n                       abline(0,0,col = 'red'),\n                       xlab = 'Predicted Values',\n                       ylab = 'Residuals',\n                       main = 'Lasso Regression Residuals')","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:24:02.695864Z","iopub.execute_input":"2021-12-24T12:24:02.697359Z","iopub.status.idle":"2021-12-24T12:24:03.127224Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"### Decision Trees","metadata":{}},{"cell_type":"code","source":"# Now we are going to try decision trees on our dataset and measure the performance of our model\n\ndtrpartfit <- train(price~.,\n               data = train,\n               method = 'rpart',\n               trControl = fit_control,\n               tuneLength =20)\n\n# Optimal cp value chosen was cp = 0.0013 \n\npreddtfit <- predict.train(dtrpartfit, newdata = test)\n\nrsq_dtrpartfit <- r_sqaured(preddtfit)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:24:03.129817Z","iopub.execute_input":"2021-12-24T12:24:03.131303Z","iopub.status.idle":"2021-12-24T12:24:30.132427Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"cat('Decision Tree R-Square - ', rsq_dtrpartfit)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:24:30.136415Z","iopub.execute_input":"2021-12-24T12:24:30.138697Z","iopub.status.idle":"2021-12-24T12:24:30.157426Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"### Random Forests","metadata":{}},{"cell_type":"code","source":"# Now we are going to try random forests on our dataset and measure the performance of our model.\n\n\nset.seed(123)\n\n\nhyper_grid <- expand.grid(mtry= seq(1,8,1),\n                          node_size = seq(5, 15, 5),\n                          sampe_size = c(0.55,0.632,0.70,0.80,1),\n                          OOB_RMSE = 0)\n\n\n\nfor (i in 1:nrow(hyper_grid)) {\n  ranger_rf <- ranger(price~., \n                      data = train, num.trees = 500, \n                      mtry = hyper_grid$mtry[i], \n                      min.node.size = hyper_grid$node_size[i], \n                      sample.fraction = hyper_grid$sampe_size[i],\n                      seed = 123)\n  \nhyper_grid$OOB_RMSE[i] <- sqrt(ranger_rf$prediction.error)\n  }\n\n# Above ranger object gives us out optimal tree of mtry = 3, node size = 5, and sample size of 100% of train data.\n \n# We use this optimal random forest tree to predict our house prices using test data\n\npred_ranger <- predict(ranger_rf, data = test)\n\nrsq_ranforfit <- r_sqaured(pred_ranger$predictions)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:24:30.170325Z","iopub.execute_input":"2021-12-24T12:24:30.171796Z","iopub.status.idle":"2021-12-24T12:26:48.416972Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"cat('Random Forest R-Square - ', rsq_ranforfit)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:26:48.419507Z","iopub.execute_input":"2021-12-24T12:26:48.421045Z","iopub.status.idle":"2021-12-24T12:26:48.435671Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"pred_plot_ranfo <- plot(x = test_response, \n                  y = pred_ranger$predictions, \n                  xlab = 'Actual Values',\n                  ylab = 'Predicted Values',\n                  main = 'Random Forest (R-Squared - 96%)',\n                  ylim = c(6,12),\n                  abline(a=0,b=1, col = 'red'))\n\nresid_ranfor <- plot(x = pred_ranger$predictions,\n                       y = pred_ranger$predictions - test_response,\n                       abline(0,0,col = 'red'),\n                       xlab = 'Predicted Values',\n                       ylab = 'Residuals',\n                       main = 'Random Forest Residuals')","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:26:48.438115Z","iopub.execute_input":"2021-12-24T12:26:48.439606Z","iopub.status.idle":"2021-12-24T12:26:48.878609Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"### Support Vector Machines - Regression","metadata":{}},{"cell_type":"code","source":"# We tune our SVR model by trying various gamma, costs and epsilon values. The code is commented because it takes around 2 hours to find optimum\n# parameters and the model was run on machine previously to find the hyperparameters\n\ngammas <- 2^(-3:3)\ncosts <- 2^(-3:3)\nepsilon <- c(0.1,0.01,0.001)\n\n#svmtune <- tune.svm(price~.,\n#                data = train,\n#                gamma = gammas,\n#                cost = costs,\n#                epsilon = epsilon)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:26:48.882559Z","iopub.execute_input":"2021-12-24T12:26:48.885256Z","iopub.status.idle":"2021-12-24T12:26:48.907713Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# SVR was trained using radial basis kernel\n\nsvmoptimal <- svm(price~., \n                  data = train, \n                  scale = T, \n                  type = 'eps-regression', \n                  kernel = 'radial', \n                  gamma = 0.125, \n                  cost=4, \n                  epsilon = 0.01 )\n\n\npredsvm <- predict(svmoptimal, newdata = test)\n\nrsq_svm <- r_sqaured(predsvm)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:26:48.911002Z","iopub.execute_input":"2021-12-24T12:26:48.912532Z","iopub.status.idle":"2021-12-24T12:26:58.513641Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"cat('Support Vector Regression R -Square: ', rsq_svm)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:26:58.516888Z","iopub.execute_input":"2021-12-24T12:26:58.518365Z","iopub.status.idle":"2021-12-24T12:26:58.532982Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"pred_plot <- plot(x = test_response, \n                  y = predsvm, \n                  xlab = 'Actual Values',\n                  ylab = 'Predicted Values',\n                  main = 'Support Vector Regression',\n                  ylim = c(6,12),\n                  abline(a=0,b=1, col = 'red'))\n\nresid_simplelr <- plot(x = predsvm,\n                       y = predsvm - test_response,\n                       abline(0,0,col = 'red'),\n                       xlab = 'Predicted Values',\n                       ylab = 'Residuals',\n                       main = 'Support Vector Regression Residuals')","metadata":{"execution":{"iopub.status.busy":"2021-12-24T12:26:58.535529Z","iopub.execute_input":"2021-12-24T12:26:58.536983Z","iopub.status.idle":"2021-12-24T12:26:58.979800Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"# As we can see, Support Vector Machine has become the strongest Jedi of all the models present here and it is also a beautiful model with its own idiosyncracies which were exploited in getting us an R-Sqaured value of around 0.97 for this specific dataset. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}